================================================================================
HADOOP STREAMING PROJECT - QUICK START (KLASTER UCZELNI)
================================================================================

1. UPLOAD PROJEKTU
   scp -r hadoop-streaming user@uczelnia:/tmp/projekt_hadoop/
   cd /tmp/projekt_hadoop

2. UPLOAD DANYCH DO HDFS
   hadoop fs -mkdir -p /user/$(whoami)/projekt1/input
   hadoop fs -put input/datasource1 /user/$(whoami)/projekt1/input/
   hadoop fs -put input/datasource4 /user/$(whoami)/projekt1/input/

3. MAPREDUCE
   cd src
   bash run_mr.sh /user/$(whoami)/projekt1/input/datasource1 \
                  /user/$(whoami)/projekt1/output_mr3

4. HIVE
   bash run_hive.sh /user/$(whoami)/projekt1/input/datasource4 \
                    /user/$(whoami)/projekt1/output_mr3 \
                    /user/$(whoami)/projekt1/output6

5. POBIERZ WYNIKI
   hadoop fs -getmerge /user/$(whoami)/projekt1/output6 output6_final.json
   cat output6_final.json | head -10

================================================================================
WERYFIKACJA:
- MapReduce output: ~300 rekordów (airport_id,status,count,avg_price)
- Hive output: ~50-70 JSON-ów (continent,country,flights,price,rank)
================================================================================

TROUBLESHOOTING:
- Jeśli Hive fails: Sprawdź czy src/hive.hql ma "SET hive.execution.engine=mr;"
- Sprawdź logi: yarn logs -applicationId <app_id>
- Web UI: http://<host>:8088

PLIKI KLUCZOWE:
✓ src/hive.hql - Inteligentne parsowanie CSV + VIEW (obsługuje błędy!)
✓ src/run_hive.sh - Używa beeline -n "$(id -un)" (z warsztatów)
✓ input/datasource4/airports.csv - 37 wadliwych rekordów (automatycznie naprawiane)

================================================================================
